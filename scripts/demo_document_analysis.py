#!/usr/bin/env python3
"""
æ–‡æ¡£åˆ†ææ¼”ç¤ºè„šæœ¬

æ¼”ç¤ºæ–‡æ¡£åˆ†æå™¨çš„å®Œæ•´åŠŸèƒ½ï¼Œå¹¶ç”Ÿæˆåˆ†ææŠ¥å‘Šã€‚
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.core.shared.utils.logger import get_logger
from app.core.document_analyzer import DocumentAnalyzer, ChunkingStrategy

logger = get_logger(__name__)


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    try:
        logger.info("ğŸš€ å¼€å§‹æ–‡æ¡£åˆ†ææ¼”ç¤º...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = project_root / "examples" / "analysis_results"
        output_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºæ–‡æ¡£åˆ†æå™¨
        analyzer = DocumentAnalyzer(config={
            "enable_validation": True,
            "enable_chunking": True,
            "max_tokens": 2000,
            "overlap_tokens": 100
        })
        
        # åˆ†æç¤ºä¾‹APIæ–‡æ¡£
        api_file = project_root / "examples" / "sample_api.json"
        
        if not api_file.exists():
            logger.error(f"ç¤ºä¾‹APIæ–‡ä»¶ä¸å­˜åœ¨: {api_file}")
            return False
        
        logger.info(f"ğŸ“„ åˆ†ææ–‡æ¡£: {api_file}")
        
        # æ‰§è¡Œåˆ†æ
        result = analyzer.analyze_file(api_file)
        
        # æ˜¾ç¤ºåˆ†æç»“æœ
        logger.info("ğŸ“Š åˆ†æç»“æœ:")
        logger.info(f"   ğŸ“‹ æ–‡æ¡£æ ‡é¢˜: {result.title}")
        logger.info(f"   ğŸ”¢ APIç‰ˆæœ¬: {result.version}")
        logger.info(f"   ğŸŒ åŸºç¡€URL: {result.base_url}")
        logger.info(f"   ğŸ”— ç«¯ç‚¹æ•°é‡: {result.total_endpoints}")
        
        if result.quality_metrics:
            logger.info(f"   â­ è´¨é‡è¯„åˆ†: {result.quality_metrics.overall_score:.1f}/100")
            logger.info(f"   ğŸ“ˆ è´¨é‡ç­‰çº§: {result.quality_metrics.quality_level}")
            logger.info(f"   âœ… å®Œæ•´æ€§: {result.quality_metrics.completeness_score:.1f}/100")
            logger.info(f"   ğŸ”„ ä¸€è‡´æ€§: {result.quality_metrics.consistency_score:.1f}/100")
            logger.info(f"   ğŸ’¡ æ¸…æ™°åº¦: {result.quality_metrics.clarity_score:.1f}/100")
            logger.info(f"   ğŸ¯ å¯ç”¨æ€§: {result.quality_metrics.usability_score:.1f}/100")
        
        logger.info(f"   âš ï¸  é—®é¢˜æ•°é‡: {len(result.issues)}")
        logger.info(f"   ğŸ“¦ åˆ†å—æ•°é‡: {len(result.chunks)}")
        logger.info(f"   â±ï¸  åˆ†æè€—æ—¶: {result.analysis_duration:.3f}ç§’")
        
        # æ˜¾ç¤ºç«¯ç‚¹ä¿¡æ¯
        logger.info("\nğŸ”— APIç«¯ç‚¹åˆ—è¡¨:")
        for i, endpoint in enumerate(result.endpoints, 1):
            logger.info(f"   {i}. {endpoint.method} {endpoint.path}")
            if endpoint.summary:
                logger.info(f"      ğŸ“ {endpoint.summary}")
            if endpoint.tags:
                logger.info(f"      ğŸ·ï¸  æ ‡ç­¾: {', '.join(endpoint.tags)}")
        
        # æ˜¾ç¤ºå‘ç°çš„é—®é¢˜
        if result.issues:
            logger.info("\nâš ï¸ å‘ç°çš„é—®é¢˜:")
            for i, issue in enumerate(result.issues, 1):
                severity_icon = {
                    "critical": "ğŸ”´",
                    "high": "ğŸŸ ", 
                    "medium": "ğŸŸ¡",
                    "low": "ğŸŸ¢",
                    "info": "â„¹ï¸"
                }.get(issue.severity, "â“")
                
                logger.info(f"   {i}. {severity_icon} [{issue.severity.upper()}] {issue.message}")
                if issue.location:
                    logger.info(f"      ğŸ“ ä½ç½®: {issue.location}")
                if issue.suggestion:
                    logger.info(f"      ğŸ’¡ å»ºè®®: {issue.suggestion}")
        
        # æ˜¾ç¤ºæ”¹è¿›å»ºè®®
        if result.suggestions:
            logger.info("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for i, suggestion in enumerate(result.suggestions, 1):
                logger.info(f"   {i}. {suggestion}")
        
        # æ˜¾ç¤ºåˆ†å—ä¿¡æ¯
        if result.chunks:
            logger.info("\nğŸ“¦ æ–‡æ¡£åˆ†å—ä¿¡æ¯:")
            for i, chunk in enumerate(result.chunks, 1):
                logger.info(f"   åˆ†å—{i}: {chunk.token_count} tokens, {len(chunk.endpoints)} endpoints")
                logger.info(f"      ç±»å‹: {chunk.chunk_type}")
                if chunk.endpoints:
                    logger.info(f"      ç«¯ç‚¹: {', '.join(chunk.endpoints)}")
        
        # ç”Ÿæˆåˆ†ææ‘˜è¦
        summary = analyzer.get_analysis_summary(result)
        logger.info("\nğŸ“ˆ åˆ†ææ‘˜è¦:")
        logger.info(f"   æ–‡æ¡£ç±»å‹: {summary['document_info']['type']}")
        logger.info(f"   HTTPæ–¹æ³•åˆ†å¸ƒ: {summary['api_info']['methods']}")
        logger.info(f"   æ ‡ç­¾åˆ†å¸ƒ: {summary['api_info']['tags']}")
        logger.info(f"   å·²å¼ƒç”¨ç«¯ç‚¹: {summary['api_info']['deprecated_count']}")
        
        # å¯¼å‡ºåˆ†ææŠ¥å‘Š
        logger.info("\nğŸ“„ å¯¼å‡ºåˆ†ææŠ¥å‘Š...")
        
        # JSONæ ¼å¼æŠ¥å‘Š
        json_report = output_dir / "analysis_report.json"
        success = analyzer.export_analysis_report(result, json_report, "json")
        if success:
            logger.info(f"   âœ… JSONæŠ¥å‘Š: {json_report}")
        else:
            logger.error(f"   âŒ JSONæŠ¥å‘Šå¯¼å‡ºå¤±è´¥")
        
        # Markdownæ ¼å¼æŠ¥å‘Š
        md_report = output_dir / "analysis_report.md"
        success = analyzer.export_analysis_report(result, md_report, "markdown")
        if success:
            logger.info(f"   âœ… MarkdownæŠ¥å‘Š: {md_report}")
        else:
            logger.error(f"   âŒ MarkdownæŠ¥å‘Šå¯¼å‡ºå¤±è´¥")
        
        logger.info("\nğŸ‰ æ–‡æ¡£åˆ†ææ¼”ç¤ºå®Œæˆï¼")
        logger.info(f"ğŸ“ æŠ¥å‘Šæ–‡ä»¶ä¿å­˜åœ¨: {output_dir}")
        
        return True
        
    except Exception as e:
        logger.error(f"ğŸ’¥ æ–‡æ¡£åˆ†ææ¼”ç¤ºå¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
