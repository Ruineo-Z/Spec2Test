"""
å¹¶å‘æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨

å®ç°åŸºäºasyncioçš„å¹¶å‘æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆï¼Œæ˜¾è‘—æå‡å¤§é‡ç«¯ç‚¹çš„ç”Ÿæˆæ•ˆç‡ã€‚
"""

import asyncio
import time
from typing import Dict, Any, List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime

from app.core.shared.llm.factory import LLMFactory
from app.core.shared.llm.base import BaseLLMClient
from app.core.shared.utils.logger import get_logger
from app.core.document_analyzer.models import DocumentAnalysisResult, APIEndpoint

from .models import (
    GenerationConfig, GenerationResult, TestSuite, TestCase, 
    TestCaseType, TestCasePriority, TestStep, TestAssertion,
    LLMGenerationResponse, LLMTestCase
)
from .prompt_builder import PromptBuilder
from .case_generator import TestCaseGenerator


class ConcurrentTestCaseGenerator(TestCaseGenerator):
    """å¹¶å‘æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨
    
    ç»§æ‰¿è‡ªåŸºç¡€ç”Ÿæˆå™¨ï¼Œæ·»åŠ å¹¶å‘å¤„ç†èƒ½åŠ›ã€‚
    """
    
    def __init__(self, llm_config: Optional[Dict[str, Any]] = None, max_workers: int = 3):
        """åˆå§‹åŒ–å¹¶å‘æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨
        
        Args:
            llm_config: LLMé…ç½®
            max_workers: æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°
        """
        super().__init__(llm_config)
        self.max_workers = max_workers
        self.logger.info(f"å¹¶å‘æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼Œæœ€å¤§å¹¶å‘æ•°: {max_workers}")
    
    def generate_test_cases_concurrent(self, 
                                     analysis_result: DocumentAnalysisResult,
                                     config: Optional[GenerationConfig] = None) -> GenerationResult:
        """å¹¶å‘ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        
        Args:
            analysis_result: æ–‡æ¡£åˆ†æç»“æœ
            config: ç”Ÿæˆé…ç½®
            
        Returns:
            GenerationResult: ç”Ÿæˆç»“æœ
        """
        start_time = time.time()
        
        try:
            self.logger.info("å¼€å§‹å¹¶å‘ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹")
            
            # ä½¿ç”¨é»˜è®¤é…ç½®
            if config is None:
                config = GenerationConfig()
            
            # åˆ›å»ºç”Ÿæˆç»“æœ
            result = GenerationResult(
                success=True,
                config_used=config,
                generated_at=datetime.now()
            )
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç«¯ç‚¹
            if not analysis_result.endpoints:
                result.add_error("æ–‡æ¡£åˆ†æç»“æœä¸­æ²¡æœ‰æ‰¾åˆ°APIç«¯ç‚¹")
                return result
            
            # åˆ›å»ºæµ‹è¯•å¥—ä»¶
            test_suite = TestSuite(
                suite_id=f"suite_{int(time.time())}",
                name=f"{analysis_result.title or 'API'} æµ‹è¯•å¥—ä»¶",
                description=f"åŸºäºæ–‡æ¡£åˆ†æç»“æœå¹¶å‘ç”Ÿæˆçš„æµ‹è¯•å¥—ä»¶",
                base_url=analysis_result.base_url
            )
            
            # å¹¶å‘ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
            endpoint_results = self._generate_concurrent(analysis_result, config)
            
            # æ±‡æ€»ç»“æœ
            llm_calls = 0
            for endpoint, cases, success in endpoint_results:
                if success:
                    for case in cases:
                        test_suite.add_test_case(case)
                    llm_calls += 1
                    self.logger.info(f"ç«¯ç‚¹ {endpoint.method} {endpoint.path} ç”Ÿæˆäº† {len(cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
                else:
                    error_msg = f"ç«¯ç‚¹ {endpoint.method} {endpoint.path} ç”Ÿæˆå¤±è´¥"
                    self.logger.error(error_msg)
                    result.add_warning(error_msg)
            
            # è®¾ç½®ç»“æœ
            result.test_suite = test_suite
            result.total_cases_generated = len(test_suite.test_cases)
            result.llm_calls_count = llm_calls
            result.generation_duration = time.time() - start_time
            
            # ç»Ÿè®¡ä¿¡æ¯
            result.cases_by_type = self._count_cases_by_type(test_suite.test_cases)
            result.cases_by_priority = self._count_cases_by_priority(test_suite.test_cases)
            
            self.logger.info(f"å¹¶å‘æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå®Œæˆ: å…±ç”Ÿæˆ {result.total_cases_generated} ä¸ªç”¨ä¾‹ï¼Œè€—æ—¶ {result.generation_duration:.2f}ç§’")
            
            return result
            
        except Exception as e:
            error_msg = f"å¹¶å‘æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå¤±è´¥: {str(e)}"
            self.logger.error(error_msg)
            
            result = GenerationResult(
                success=False,
                generation_duration=time.time() - start_time,
                config_used=config
            )
            result.add_error(error_msg)
            return result
    
    def _generate_concurrent(self, 
                           analysis_result: DocumentAnalysisResult,
                           config: GenerationConfig) -> List[tuple]:
        """å¹¶å‘ç”Ÿæˆæ‰€æœ‰ç«¯ç‚¹çš„æµ‹è¯•ç”¨ä¾‹
        
        Args:
            analysis_result: æ–‡æ¡£åˆ†æç»“æœ
            config: ç”Ÿæˆé…ç½®
            
        Returns:
            List[tuple]: (endpoint, cases, success) çš„åˆ—è¡¨
        """
        results = []
        
        # ä½¿ç”¨ThreadPoolExecutorè¿›è¡Œå¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_endpoint = {
                executor.submit(
                    self._generate_single_endpoint_safe,
                    analysis_result, endpoint, config
                ): endpoint
                for endpoint in analysis_result.endpoints
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_endpoint):
                endpoint = future_to_endpoint[future]
                try:
                    cases = future.result()
                    results.append((endpoint, cases, True))
                except Exception as e:
                    self.logger.error(f"ç«¯ç‚¹ {endpoint.method} {endpoint.path} å¹¶å‘ç”Ÿæˆå¤±è´¥: {e}")
                    results.append((endpoint, [], False))
        
        return results
    
    def _generate_single_endpoint_safe(self, 
                                     analysis_result: DocumentAnalysisResult,
                                     endpoint: APIEndpoint,
                                     config: GenerationConfig) -> List[TestCase]:
        """å®‰å…¨åœ°ä¸ºå•ä¸ªç«¯ç‚¹ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼ˆç”¨äºå¹¶å‘è°ƒç”¨ï¼‰
        
        Args:
            analysis_result: æ–‡æ¡£åˆ†æç»“æœ
            endpoint: APIç«¯ç‚¹
            config: ç”Ÿæˆé…ç½®
            
        Returns:
            List[TestCase]: ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
        """
        try:
            # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„LLMå®¢æˆ·ç«¯
            thread_llm_client = LLMFactory.create_client("ollama", {
                "base_url": "http://localhost:11434",
                "model": "qwen2.5:3b",
                "timeout": 300,
                "temperature": 0.3
            })
            
            # æ„å»ºæç¤ºè¯
            prompt = self.prompt_builder.build_test_generation_prompt(
                analysis_result, endpoint, config
            )
            
            # è°ƒç”¨LLMç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
            llm_response = thread_llm_client.generate_text(
                prompt=prompt,
                schema=LLMGenerationResponse.model_json_schema()
            )
            
            # è§£æLLMå“åº”
            generation_response = self._parse_llm_response(llm_response)
            
            # è½¬æ¢ä¸ºæ ‡å‡†æµ‹è¯•ç”¨ä¾‹æ ¼å¼
            test_cases = self._convert_llm_cases_to_test_cases(
                generation_response.test_cases, endpoint, config
            )
            
            return test_cases
            
        except Exception as e:
            self.logger.error(f"ä¸ºç«¯ç‚¹ {endpoint.method} {endpoint.path} ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å¤±è´¥: {e}")
            return []


class AdaptiveTestCaseGenerator(TestCaseGenerator):
    """è‡ªé€‚åº”æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨

    æ ¹æ®ç«¯ç‚¹æ•°é‡å’ŒLLMæœåŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç­–ç•¥ã€‚
    """

    def __init__(self, llm_config: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ–è‡ªé€‚åº”ç”Ÿæˆå™¨"""
        super().__init__(llm_config)

        # ä»é…ç½®è·å–åŸºç¡€å¹¶å‘è®¾ç½®
        try:
            from app.config import get_performance_config
            perf_config = get_performance_config()
            base_max_workers = perf_config.max_concurrent_workers
            base_threshold = perf_config.concurrent_threshold
        except ImportError:
            # å¦‚æœæ— æ³•å¯¼å…¥é…ç½®ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡
            import os
            base_max_workers = int(os.getenv("SPEC2TEST_MAX_CONCURRENT_WORKERS", "8"))
            base_threshold = int(os.getenv("SPEC2TEST_CONCURRENT_THRESHOLD", "3"))

        # æ ¹æ®LLMç±»å‹è°ƒæ•´ç­–ç•¥
        provider = (llm_config or {}).get("provider", "ollama")
        if provider == "ollama":
            # Ollamaæœ¬åœ°æœåŠ¡ï¼šä¿å®ˆçš„å¹¶å‘ç­–ç•¥
            self.concurrent_threshold = max(base_threshold, 5)  # è‡³å°‘5ä¸ªç«¯ç‚¹æ‰å¹¶å‘
            self.max_workers = min(base_max_workers, 3)  # æœ€å¤š3ä¸ªå¹¶å‘
        elif provider in ["openai", "claude", "gemini"]:
            # äº‘ç«¯æœåŠ¡ï¼šä½¿ç”¨é…ç½®çš„å¹¶å‘ç­–ç•¥
            self.concurrent_threshold = base_threshold
            self.max_workers = base_max_workers
        else:
            # æœªçŸ¥æœåŠ¡ï¼šä¸­ç­‰ç­–ç•¥
            self.concurrent_threshold = base_threshold + 1
            self.max_workers = min(base_max_workers, 5)

        self.concurrent_generator = ConcurrentTestCaseGenerator(llm_config, self.max_workers)

        self.logger.info(f"è‡ªé€‚åº”ç”Ÿæˆå™¨é…ç½®: é˜ˆå€¼={self.concurrent_threshold}, æœ€å¤§å¹¶å‘={self.max_workers} (åŸºäº{provider}æä¾›å•†)")
        
    def generate_test_cases_adaptive(self, 
                                   analysis_result: DocumentAnalysisResult,
                                   config: Optional[GenerationConfig] = None) -> GenerationResult:
        """è‡ªé€‚åº”ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        
        Args:
            analysis_result: æ–‡æ¡£åˆ†æç»“æœ
            config: ç”Ÿæˆé…ç½®
            
        Returns:
            GenerationResult: ç”Ÿæˆç»“æœ
        """
        endpoint_count = len(analysis_result.endpoints)
        
        if endpoint_count >= self.concurrent_threshold:
            self.logger.info(f"ç«¯ç‚¹æ•°é‡ {endpoint_count} >= {self.concurrent_threshold}ï¼Œä½¿ç”¨å¹¶å‘æ¨¡å¼")
            return self.concurrent_generator.generate_test_cases_concurrent(analysis_result, config)
        else:
            self.logger.info(f"ç«¯ç‚¹æ•°é‡ {endpoint_count} < {self.concurrent_threshold}ï¼Œä½¿ç”¨ä¸²è¡Œæ¨¡å¼")
            return self.generate_test_cases(analysis_result, config)


# æ€§èƒ½å¯¹æ¯”å·¥å…·
class PerformanceComparator:
    """æ€§èƒ½å¯¹æ¯”å·¥å…·"""
    
    @staticmethod
    def compare_serial_vs_concurrent(analysis_result: DocumentAnalysisResult,
                                   config: Optional[GenerationConfig] = None):
        """å¯¹æ¯”ä¸²è¡Œå’Œå¹¶å‘æ€§èƒ½
        
        Args:
            analysis_result: æ–‡æ¡£åˆ†æç»“æœ
            config: ç”Ÿæˆé…ç½®
        """
        logger = get_logger("PerformanceComparator")
        
        # ä¸²è¡Œæµ‹è¯•
        logger.info("ğŸ”„ å¼€å§‹ä¸²è¡Œç”Ÿæˆæµ‹è¯•...")
        serial_generator = TestCaseGenerator()
        serial_start = time.time()
        serial_result = serial_generator.generate_test_cases(analysis_result, config)
        serial_duration = time.time() - serial_start
        
        # å¹¶å‘æµ‹è¯•
        logger.info("âš¡ å¼€å§‹å¹¶å‘ç”Ÿæˆæµ‹è¯•...")
        concurrent_generator = ConcurrentTestCaseGenerator()
        concurrent_start = time.time()
        concurrent_result = concurrent_generator.generate_test_cases_concurrent(analysis_result, config)
        concurrent_duration = time.time() - concurrent_start
        
        # æ€§èƒ½å¯¹æ¯”
        improvement = (serial_duration - concurrent_duration) / serial_duration * 100
        
        logger.info("ğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ:")
        logger.info(f"   ä¸²è¡Œæ¨¡å¼: {serial_duration:.2f}ç§’, {serial_result.total_cases_generated}ä¸ªç”¨ä¾‹")
        logger.info(f"   å¹¶å‘æ¨¡å¼: {concurrent_duration:.2f}ç§’, {concurrent_result.total_cases_generated}ä¸ªç”¨ä¾‹")
        logger.info(f"   æ€§èƒ½æå‡: {improvement:.1f}%")
        
        return {
            "serial_duration": serial_duration,
            "concurrent_duration": concurrent_duration,
            "improvement_percent": improvement,
            "serial_cases": serial_result.total_cases_generated,
            "concurrent_cases": concurrent_result.total_cases_generated
        }
