"""
æ–‡æ¡£åˆ†æå™¨ä¸»ç±»

æ•´åˆæ–‡æ¡£è§£æã€è´¨é‡æ£€æŸ¥ã€åˆ†å—ç­‰åŠŸèƒ½çš„ä¸»è¦åˆ†æå™¨ã€‚
"""

import time
from typing import Dict, Any, Optional, Union
from pathlib import Path

from .models import DocumentAnalysisResult, ChunkingStrategy
from .parser import DocumentParser
from .validator import DocumentValidator
from .chunker import DocumentChunker
from app.core.shared.utils.logger import get_logger

logger = get_logger(__name__)


class DocumentAnalyzer:
    """æ–‡æ¡£åˆ†æå™¨ä¸»ç±»
    
    æä¾›å®Œæ•´çš„APIæ–‡æ¡£åˆ†æåŠŸèƒ½ï¼ŒåŒ…æ‹¬è§£æã€è´¨é‡æ£€æŸ¥ã€åˆ†å—ç­‰ã€‚
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ–æ–‡æ¡£åˆ†æå™¨
        
        Args:
            config: åˆ†æå™¨é…ç½®
        """
        self.config = config or {}
        self.logger = get_logger(f"{self.__class__.__name__}")
        
        # åˆå§‹åŒ–å­ç»„ä»¶
        self.parser = DocumentParser()
        self.validator = DocumentValidator()
        self.chunker = DocumentChunker()
        
        # é…ç½®å‚æ•°
        self.enable_validation = self.config.get("enable_validation", True)
        self.enable_chunking = self.config.get("enable_chunking", False)
        self.default_chunking_strategy = ChunkingStrategy(
            max_tokens=self.config.get("max_tokens", 4000),
            overlap_tokens=self.config.get("overlap_tokens", 200),
            chunk_by_endpoint=self.config.get("chunk_by_endpoint", True)
        )
        
        self.logger.info("æ–‡æ¡£åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_file(self, file_path: Union[str, Path], 
                    chunking_strategy: Optional[ChunkingStrategy] = None) -> DocumentAnalysisResult:
        """åˆ†ææ–‡æ¡£æ–‡ä»¶
        
        Args:
            file_path: æ–‡æ¡£æ–‡ä»¶è·¯å¾„
            chunking_strategy: åˆ†å—ç­–ç•¥ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            DocumentAnalysisResult: åˆ†æç»“æœ
        """
        try:
            file_path = Path(file_path)
            
            if not file_path.exists():
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            self.logger.info(f"å¼€å§‹åˆ†ææ–‡æ¡£æ–‡ä»¶: {file_path}")
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åˆ†æå†…å®¹
            return self.analyze_content(content, str(file_path), chunking_strategy)
            
        except Exception as e:
            error_msg = f"æ–‡æ¡£æ–‡ä»¶åˆ†æå¤±è´¥: {str(e)}"
            self.logger.error(error_msg)
            
            # è¿”å›é”™è¯¯ç»“æœ
            from .models import DocumentIssue, IssueType, IssueSeverity

            result = DocumentAnalysisResult(
                document_type="unknown",
                document_format="text"
            )
            result.issues.append(
                DocumentIssue(
                    type=IssueType.INVALID_FORMAT,
                    severity=IssueSeverity.CRITICAL,
                    message=error_msg,
                    suggestion="è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼æ˜¯å¦æ­£ç¡®"
                )
            )
            
            return result
    
    def analyze_content(self, content: str, 
                       source_name: Optional[str] = None,
                       chunking_strategy: Optional[ChunkingStrategy] = None) -> DocumentAnalysisResult:
        """åˆ†ææ–‡æ¡£å†…å®¹
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            source_name: æ¥æºåç§°ï¼ˆå¯é€‰ï¼‰
            chunking_strategy: åˆ†å—ç­–ç•¥ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            DocumentAnalysisResult: åˆ†æç»“æœ
        """
        start_time = time.time()
        
        try:
            self.logger.info(f"å¼€å§‹åˆ†ææ–‡æ¡£å†…å®¹: {source_name or 'content'}")
            
            # 1. è§£ææ–‡æ¡£
            parsed_doc = self.parser.parse_document(content, source_name)
            
            if not parsed_doc.is_valid:
                self.logger.warning(f"æ–‡æ¡£è§£ææœ‰é”™è¯¯: {parsed_doc.parse_errors}")
            
            # 2. æå–APIä¿¡æ¯
            analysis_result = self.parser.extract_api_info(parsed_doc)
            
            # 3. è´¨é‡æ£€æŸ¥
            if self.enable_validation:
                analysis_result = self.validator.validate_document(analysis_result)
            
            # 4. æ–‡æ¡£åˆ†å—
            if self.enable_chunking:
                strategy = chunking_strategy or self.default_chunking_strategy
                analysis_result = self.chunker.chunk_document(analysis_result, strategy)
            
            # 5. è®¾ç½®åˆ†ææ—¶é—´
            end_time = time.time()
            analysis_result.analysis_duration = end_time - start_time
            
            self.logger.info(f"æ–‡æ¡£åˆ†æå®Œæˆ: è€—æ—¶{analysis_result.analysis_duration:.2f}ç§’")
            
            return analysis_result
            
        except Exception as e:
            error_msg = f"æ–‡æ¡£å†…å®¹åˆ†æå¤±è´¥: {str(e)}"
            self.logger.error(error_msg)
            
            # è¿”å›é”™è¯¯ç»“æœ
            from .models import DocumentIssue, IssueType, IssueSeverity

            result = DocumentAnalysisResult(
                document_type="unknown",
                document_format="text"
            )
            result.issues.append(
                DocumentIssue(
                    type=IssueType.INVALID_FORMAT,
                    severity=IssueSeverity.CRITICAL,
                    message=error_msg,
                    suggestion="è¯·æ£€æŸ¥æ–‡æ¡£å†…å®¹æ ¼å¼æ˜¯å¦æ­£ç¡®"
                )
            )
            
            end_time = time.time()
            result.analysis_duration = end_time - start_time
            
            return result
    
    def analyze_url(self, url: str, 
                   chunking_strategy: Optional[ChunkingStrategy] = None) -> DocumentAnalysisResult:
        """åˆ†æåœ¨çº¿æ–‡æ¡£
        
        Args:
            url: æ–‡æ¡£URL
            chunking_strategy: åˆ†å—ç­–ç•¥ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            DocumentAnalysisResult: åˆ†æç»“æœ
        """
        try:
            self.logger.info(f"å¼€å§‹åˆ†æåœ¨çº¿æ–‡æ¡£: {url}")
            
            # ä½¿ç”¨HTTPå®¢æˆ·ç«¯è·å–å†…å®¹
            from app.core.shared.http import HTTPClient
            
            http_client = HTTPClient()
            response = http_client.get(url)
            
            if not response.success:
                raise Exception(f"æ— æ³•è·å–æ–‡æ¡£å†…å®¹: {response.error_message}")
            
            content = response.content
            if not content:
                raise Exception("æ–‡æ¡£å†…å®¹ä¸ºç©º")
            
            # åˆ†æå†…å®¹
            return self.analyze_content(content, url, chunking_strategy)
            
        except Exception as e:
            error_msg = f"åœ¨çº¿æ–‡æ¡£åˆ†æå¤±è´¥: {str(e)}"
            self.logger.error(error_msg)
            
            # è¿”å›é”™è¯¯ç»“æœ
            from .models import DocumentIssue, IssueType, IssueSeverity

            result = DocumentAnalysisResult(
                document_type="unknown",
                document_format="text"
            )
            result.issues.append(
                DocumentIssue(
                    type=IssueType.INVALID_FORMAT,
                    severity=IssueSeverity.CRITICAL,
                    message=error_msg,
                    suggestion="è¯·æ£€æŸ¥URLæ˜¯å¦å¯è®¿é—®"
                )
            )
            
            return result
    
    def get_analysis_summary(self, result: DocumentAnalysisResult) -> Dict[str, Any]:
        """è·å–åˆ†ææ‘˜è¦
        
        Args:
            result: åˆ†æç»“æœ
            
        Returns:
            Dict[str, Any]: åˆ†ææ‘˜è¦
        """
        summary = {
            "document_info": {
                "type": result.document_type,
                "format": result.document_format,
                "title": result.title,
                "version": result.version,
                "base_url": result.base_url
            },
            "api_info": {
                "total_endpoints": result.total_endpoints,
                "methods": self._get_method_distribution(result),
                "tags": self._get_tag_distribution(result),
                "deprecated_count": len(result.get_deprecated_endpoints())
            },
            "quality_info": {
                "overall_score": result.quality_metrics.overall_score if result.quality_metrics else 0,
                "quality_level": result.quality_metrics.quality_level if result.quality_metrics else "unknown",
                "total_issues": len(result.issues),
                "critical_issues": len(result.critical_issues),
                "high_issues": len(result.high_issues)
            },
            "analysis_info": {
                "analysis_duration": result.analysis_duration,
                "chunk_count": len(result.chunks),
                "suggestions_count": len(result.suggestions)
            }
        }
        
        return summary
    
    def _get_method_distribution(self, result: DocumentAnalysisResult) -> Dict[str, int]:
        """è·å–HTTPæ–¹æ³•åˆ†å¸ƒ"""
        method_count = {}
        for endpoint in result.endpoints:
            method = endpoint.method.upper()
            method_count[method] = method_count.get(method, 0) + 1
        
        return method_count
    
    def _get_tag_distribution(self, result: DocumentAnalysisResult) -> Dict[str, int]:
        """è·å–æ ‡ç­¾åˆ†å¸ƒ"""
        tag_count = {}
        for endpoint in result.endpoints:
            for tag in endpoint.tags:
                tag_count[tag] = tag_count.get(tag, 0) + 1
        
        return tag_count
    
    def export_analysis_report(self, result: DocumentAnalysisResult, 
                              output_path: Union[str, Path],
                              format: str = "json") -> bool:
        """å¯¼å‡ºåˆ†ææŠ¥å‘Š
        
        Args:
            result: åˆ†æç»“æœ
            output_path: è¾“å‡ºè·¯å¾„
            format: å¯¼å‡ºæ ¼å¼ï¼ˆjson/markdownï¼‰
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            output_path = Path(output_path)
            
            if format.lower() == "json":
                return self._export_json_report(result, output_path)
            elif format.lower() == "markdown":
                return self._export_markdown_report(result, output_path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format}")
                
        except Exception as e:
            self.logger.error(f"å¯¼å‡ºåˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            return False
    
    def _export_json_report(self, result: DocumentAnalysisResult, output_path: Path) -> bool:
        """å¯¼å‡ºJSONæ ¼å¼æŠ¥å‘Š"""
        import json
        
        try:
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
            report_data = result.model_dump()
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False, default=str)
            
            self.logger.info(f"JSONæŠ¥å‘Šå¯¼å‡ºæˆåŠŸ: {output_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"JSONæŠ¥å‘Šå¯¼å‡ºå¤±è´¥: {e}")
            return False
    
    def _export_markdown_report(self, result: DocumentAnalysisResult, output_path: Path) -> bool:
        """å¯¼å‡ºMarkdownæ ¼å¼æŠ¥å‘Š"""
        try:
            content = []
            
            # æ ‡é¢˜
            content.append(f"# APIæ–‡æ¡£åˆ†ææŠ¥å‘Š")
            content.append(f"**åˆ†ææ—¶é—´**: {result.analysis_timestamp}")
            content.append("")
            
            # åŸºæœ¬ä¿¡æ¯
            content.append("## åŸºæœ¬ä¿¡æ¯")
            content.append(f"- **æ–‡æ¡£ç±»å‹**: {result.document_type}")
            content.append(f"- **æ–‡æ¡£æ ¼å¼**: {result.document_format}")
            if result.title:
                content.append(f"- **æ ‡é¢˜**: {result.title}")
            if result.version:
                content.append(f"- **ç‰ˆæœ¬**: {result.version}")
            if result.base_url:
                content.append(f"- **åŸºç¡€URL**: {result.base_url}")
            content.append("")
            
            # APIç»Ÿè®¡
            content.append("## APIç»Ÿè®¡")
            content.append(f"- **ç«¯ç‚¹æ€»æ•°**: {result.total_endpoints}")
            
            method_dist = self._get_method_distribution(result)
            if method_dist:
                content.append("- **HTTPæ–¹æ³•åˆ†å¸ƒ**:")
                for method, count in method_dist.items():
                    content.append(f"  - {method}: {count}")
            content.append("")
            
            # è´¨é‡è¯„ä¼°
            if result.quality_metrics:
                content.append("## è´¨é‡è¯„ä¼°")
                content.append(f"- **æ€»ä½“è¯„åˆ†**: {result.quality_metrics.overall_score:.1f}/100")
                content.append(f"- **è´¨é‡ç­‰çº§**: {result.quality_metrics.quality_level}")
                content.append(f"- **å®Œæ•´æ€§**: {result.quality_metrics.completeness_score:.1f}/100")
                content.append(f"- **ä¸€è‡´æ€§**: {result.quality_metrics.consistency_score:.1f}/100")
                content.append("")
            
            # é—®é¢˜åˆ—è¡¨
            if result.issues:
                content.append("## å‘ç°çš„é—®é¢˜")
                for issue in result.issues:
                    severity_icon = {
                        "critical": "ğŸ”´",
                        "high": "ğŸŸ ", 
                        "medium": "ğŸŸ¡",
                        "low": "ğŸŸ¢",
                        "info": "â„¹ï¸"
                    }.get(issue.severity, "â“")
                    
                    content.append(f"### {severity_icon} {issue.message}")
                    if issue.location:
                        content.append(f"**ä½ç½®**: {issue.location}")
                    if issue.suggestion:
                        content.append(f"**å»ºè®®**: {issue.suggestion}")
                    content.append("")
            
            # æ”¹è¿›å»ºè®®
            if result.suggestions:
                content.append("## æ”¹è¿›å»ºè®®")
                for i, suggestion in enumerate(result.suggestions, 1):
                    content.append(f"{i}. {suggestion}")
                content.append("")
            
            # å†™å…¥æ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(content))
            
            self.logger.info(f"MarkdownæŠ¥å‘Šå¯¼å‡ºæˆåŠŸ: {output_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"MarkdownæŠ¥å‘Šå¯¼å‡ºå¤±è´¥: {e}")
            return False
