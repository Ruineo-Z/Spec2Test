"""
æ–‡æ¡£ç›¸å…³APIç«¯ç‚¹

æä¾›APIæ–‡æ¡£ä¸Šä¼ ã€åˆ†æå’Œç®¡ç†åŠŸèƒ½ã€‚
"""

import os
import uuid
from typing import Dict, Any, List, Optional
from datetime import datetime

from fastapi import APIRouter, UploadFile, File, HTTPException, Depends, BackgroundTasks
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field

from app.core.shared.utils.logger import get_logger
from app.core.shared.utils.exceptions import BusinessException, ValidationException
from app.core.document_analyzer import DocumentAnalyzer, AnalysisConfig
from app.core.document_analyzer.models import DocumentAnalysisResult


# åˆ›å»ºè·¯ç”±å™¨
router = APIRouter(prefix="/documents", tags=["æ–‡æ¡£"])
logger = get_logger(__name__)


# è¯·æ±‚/å“åº”æ¨¡å‹
class DocumentUploadResponse(BaseModel):
    """æ–‡æ¡£ä¸Šä¼ å“åº”"""
    document_id: str = Field(description="æ–‡æ¡£ID")
    filename: str = Field(description="æ–‡ä»¶å")
    file_size: int = Field(description="æ–‡ä»¶å¤§å°(å­—èŠ‚)")
    content_type: str = Field(description="æ–‡ä»¶ç±»å‹")
    upload_time: datetime = Field(description="ä¸Šä¼ æ—¶é—´")
    status: str = Field(description="çŠ¶æ€", default="uploaded")


class DocumentInfo(BaseModel):
    """æ–‡æ¡£ä¿¡æ¯"""
    document_id: str = Field(description="æ–‡æ¡£ID")
    filename: str = Field(description="æ–‡ä»¶å")
    file_size: int = Field(description="æ–‡ä»¶å¤§å°(å­—èŠ‚)")
    content_type: str = Field(description="æ–‡ä»¶ç±»å‹")
    upload_time: datetime = Field(description="ä¸Šä¼ æ—¶é—´")
    status: str = Field(description="çŠ¶æ€")
    analysis_result: Optional[Dict[str, Any]] = Field(default=None, description="åˆ†æç»“æœ")


class DocumentAnalysisRequest(BaseModel):
    """æ–‡æ¡£åˆ†æè¯·æ±‚"""
    config: Optional[Dict[str, Any]] = Field(default=None, description="åˆ†æé…ç½®")
    force_reanalyze: bool = Field(default=False, description="å¼ºåˆ¶é‡æ–°åˆ†æ")


class DocumentAnalysisResponse(BaseModel):
    """æ–‡æ¡£åˆ†æå“åº”"""
    document_id: str = Field(description="æ–‡æ¡£ID")
    analysis_id: str = Field(description="åˆ†æID")
    status: str = Field(description="åˆ†æçŠ¶æ€")
    started_at: datetime = Field(description="å¼€å§‹æ—¶é—´")
    message: str = Field(description="çŠ¶æ€æ¶ˆæ¯")


# ç®€å•çš„å†…å­˜å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨æ•°æ®åº“ï¼‰
documents_store: Dict[str, Dict[str, Any]] = {}
analysis_results_store: Dict[str, DocumentAnalysisResult] = {}


@router.post("/", response_model=DocumentUploadResponse, summary="ä¸Šä¼ APIæ–‡æ¡£")
async def upload_document(
    file: UploadFile = File(..., description="APIæ–‡æ¡£æ–‡ä»¶")
) -> DocumentUploadResponse:
    """
    ä¸Šä¼ APIæ–‡æ¡£æ–‡ä»¶
    
    æ”¯æŒçš„æ–‡ä»¶æ ¼å¼:
    - JSONæ ¼å¼çš„OpenAPI 3.0è§„èŒƒæ–‡æ¡£
    - YAMLæ ¼å¼çš„OpenAPI 3.0è§„èŒƒæ–‡æ¡£
    
    æ–‡ä»¶å¤§å°é™åˆ¶: 10MB
    """
    logger.info(f"ğŸ“¤ æ¥æ”¶æ–‡æ¡£ä¸Šä¼ è¯·æ±‚: {file.filename}")
    
    # éªŒè¯æ–‡ä»¶
    if not file.filename:
        raise HTTPException(status_code=400, detail="æ–‡ä»¶åä¸èƒ½ä¸ºç©º")
    
    # æ£€æŸ¥æ–‡ä»¶ç±»å‹
    allowed_types = ["application/json", "text/yaml", "application/x-yaml", "text/plain"]
    if file.content_type not in allowed_types:
        raise HTTPException(
            status_code=400, 
            detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file.content_type}ã€‚æ”¯æŒçš„ç±»å‹: {', '.join(allowed_types)}"
        )
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    max_size = 10 * 1024 * 1024  # 10MB
    content = await file.read()
    if len(content) > max_size:
        raise HTTPException(
            status_code=413, 
            detail=f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ ({max_size} bytes)"
        )
    
    # ç”Ÿæˆæ–‡æ¡£ID
    document_id = str(uuid.uuid4())
    
    # ä¿å­˜æ–‡æ¡£ä¿¡æ¯
    document_info = {
        "document_id": document_id,
        "filename": file.filename,
        "file_size": len(content),
        "content_type": file.content_type,
        "upload_time": datetime.now(),
        "status": "uploaded",
        "content": content.decode('utf-8'),
        "analysis_result": None
    }
    
    documents_store[document_id] = document_info
    
    logger.info(f"âœ… æ–‡æ¡£ä¸Šä¼ æˆåŠŸ: {document_id} ({file.filename})")
    
    return DocumentUploadResponse(
        document_id=document_id,
        filename=file.filename,
        file_size=len(content),
        content_type=file.content_type,
        upload_time=document_info["upload_time"],
        status="uploaded"
    )


@router.get("/{document_id}", response_model=DocumentInfo, summary="è·å–æ–‡æ¡£ä¿¡æ¯")
async def get_document_info(document_id: str) -> DocumentInfo:
    """
    è·å–æŒ‡å®šæ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
    
    åŒ…æ‹¬æ–‡æ¡£åŸºæœ¬ä¿¡æ¯å’Œåˆ†æç»“æœï¼ˆå¦‚æœå·²åˆ†æï¼‰
    """
    logger.info(f"ğŸ“‹ è·å–æ–‡æ¡£ä¿¡æ¯: {document_id}")
    
    # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
    if document_id not in documents_store:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    doc_info = documents_store[document_id]
    
    # æ„å»ºå“åº”
    response = DocumentInfo(
        document_id=doc_info["document_id"],
        filename=doc_info["filename"],
        file_size=doc_info["file_size"],
        content_type=doc_info["content_type"],
        upload_time=doc_info["upload_time"],
        status=doc_info["status"],
        analysis_result=doc_info.get("analysis_result")
    )
    
    logger.info(f"âœ… æ–‡æ¡£ä¿¡æ¯è·å–æˆåŠŸ: {document_id}")
    return response


@router.post("/{document_id}/analyze", response_model=DocumentAnalysisResponse, summary="åˆ†æAPIæ–‡æ¡£")
async def analyze_document(
    document_id: str,
    request: DocumentAnalysisRequest,
    background_tasks: BackgroundTasks
) -> DocumentAnalysisResponse:
    """
    åˆ†æAPIæ–‡æ¡£ï¼Œæå–ç«¯ç‚¹ä¿¡æ¯å’Œæ•°æ®æ¨¡å‹
    
    åˆ†æè¿‡ç¨‹åŒ…æ‹¬:
    1. æ–‡æ¡£æ ¼å¼éªŒè¯
    2. OpenAPIè§„èŒƒè§£æ
    3. ç«¯ç‚¹ä¿¡æ¯æå–
    4. æ•°æ®æ¨¡å‹åˆ†æ
    5. è´¨é‡è¯„ä¼°
    """
    logger.info(f"ğŸ” å¼€å§‹åˆ†ææ–‡æ¡£: {document_id}")
    
    # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
    if document_id not in documents_store:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    doc_info = documents_store[document_id]
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åˆ†æ
    if not request.force_reanalyze and doc_info.get("analysis_result"):
        logger.info(f"ğŸ“‹ æ–‡æ¡£å·²åˆ†æï¼Œè¿”å›ç°æœ‰ç»“æœ: {document_id}")
        return DocumentAnalysisResponse(
            document_id=document_id,
            analysis_id=doc_info["analysis_result"]["analysis_id"],
            status="completed",
            started_at=doc_info["analysis_result"]["started_at"],
            message="æ–‡æ¡£å·²åˆ†æå®Œæˆ"
        )
    
    # ç”Ÿæˆåˆ†æID
    analysis_id = str(uuid.uuid4())
    
    # æ›´æ–°æ–‡æ¡£çŠ¶æ€
    doc_info["status"] = "analyzing"
    doc_info["analysis_id"] = analysis_id
    
    # åˆ›å»ºåˆ†æé…ç½®
    config = AnalysisConfig()
    if request.config:
        # åº”ç”¨ç”¨æˆ·é…ç½®
        for key, value in request.config.items():
            if hasattr(config, key):
                setattr(config, key, value)
    
    # æ·»åŠ åå°åˆ†æä»»åŠ¡
    background_tasks.add_task(
        _perform_document_analysis,
        document_id,
        analysis_id,
        doc_info["content"],
        config
    )
    
    logger.info(f"ğŸš€ æ–‡æ¡£åˆ†æä»»åŠ¡å·²å¯åŠ¨: {document_id} -> {analysis_id}")
    
    return DocumentAnalysisResponse(
        document_id=document_id,
        analysis_id=analysis_id,
        status="analyzing",
        started_at=datetime.now(),
        message="æ–‡æ¡£åˆ†æå·²å¼€å§‹ï¼Œè¯·ç¨åæŸ¥è¯¢ç»“æœ"
    )


@router.get("/{document_id}/analysis", summary="è·å–æ–‡æ¡£åˆ†æç»“æœ")
async def get_document_analysis(document_id: str) -> Dict[str, Any]:
    """
    è·å–æ–‡æ¡£åˆ†æç»“æœ
    
    è¿”å›è¯¦ç»†çš„åˆ†æç»“æœï¼ŒåŒ…æ‹¬ç«¯ç‚¹ä¿¡æ¯ã€æ•°æ®æ¨¡å‹ã€è´¨é‡è¯„ä¼°ç­‰
    """
    logger.info(f"ğŸ“Š è·å–æ–‡æ¡£åˆ†æç»“æœ: {document_id}")
    
    # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
    if document_id not in documents_store:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    doc_info = documents_store[document_id]
    
    # æ£€æŸ¥æ˜¯å¦å·²åˆ†æ
    if not doc_info.get("analysis_result"):
        raise HTTPException(status_code=404, detail="æ–‡æ¡£å°šæœªåˆ†ææˆ–åˆ†æå¤±è´¥")
    
    analysis_result = doc_info["analysis_result"]
    
    logger.info(f"âœ… åˆ†æç»“æœè·å–æˆåŠŸ: {document_id}")
    
    return {
        "document_id": document_id,
        "analysis_id": analysis_result["analysis_id"],
        "status": analysis_result["status"],
        "started_at": analysis_result["started_at"],
        "completed_at": analysis_result.get("completed_at"),
        "result": analysis_result.get("result"),
        "error": analysis_result.get("error")
    }


@router.delete("/{document_id}", summary="åˆ é™¤æ–‡æ¡£")
async def delete_document(document_id: str) -> Dict[str, Any]:
    """
    åˆ é™¤æŒ‡å®šçš„æ–‡æ¡£åŠå…¶åˆ†æç»“æœ
    """
    logger.info(f"ğŸ—‘ï¸ åˆ é™¤æ–‡æ¡£: {document_id}")
    
    # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
    if document_id not in documents_store:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    # åˆ é™¤æ–‡æ¡£
    del documents_store[document_id]
    
    # åˆ é™¤åˆ†æç»“æœ
    if document_id in analysis_results_store:
        del analysis_results_store[document_id]
    
    logger.info(f"âœ… æ–‡æ¡£åˆ é™¤æˆåŠŸ: {document_id}")
    
    return {
        "message": "æ–‡æ¡£åˆ é™¤æˆåŠŸ",
        "document_id": document_id,
        "deleted_at": datetime.now()
    }


@router.get("/", summary="è·å–æ–‡æ¡£åˆ—è¡¨")
async def list_documents(
    limit: int = 10,
    offset: int = 0,
    status: Optional[str] = None
) -> Dict[str, Any]:
    """
    è·å–æ–‡æ¡£åˆ—è¡¨
    
    æ”¯æŒåˆ†é¡µå’ŒçŠ¶æ€è¿‡æ»¤
    """
    logger.info(f"ğŸ“‹ è·å–æ–‡æ¡£åˆ—è¡¨: limit={limit}, offset={offset}, status={status}")
    
    # è·å–æ‰€æœ‰æ–‡æ¡£
    all_docs = list(documents_store.values())
    
    # çŠ¶æ€è¿‡æ»¤
    if status:
        all_docs = [doc for doc in all_docs if doc["status"] == status]
    
    # æ’åºï¼ˆæŒ‰ä¸Šä¼ æ—¶é—´å€’åºï¼‰
    all_docs.sort(key=lambda x: x["upload_time"], reverse=True)
    
    # åˆ†é¡µ
    total = len(all_docs)
    docs = all_docs[offset:offset + limit]
    
    # æ„å»ºå“åº”
    documents = []
    for doc in docs:
        documents.append({
            "document_id": doc["document_id"],
            "filename": doc["filename"],
            "file_size": doc["file_size"],
            "content_type": doc["content_type"],
            "upload_time": doc["upload_time"],
            "status": doc["status"]
        })
    
    logger.info(f"âœ… æ–‡æ¡£åˆ—è¡¨è·å–æˆåŠŸ: {len(documents)}/{total}")
    
    return {
        "documents": documents,
        "total": total,
        "limit": limit,
        "offset": offset,
        "has_more": offset + limit < total
    }


async def _perform_document_analysis(
    document_id: str,
    analysis_id: str,
    content: str,
    config: AnalysisConfig
):
    """æ‰§è¡Œæ–‡æ¡£åˆ†æï¼ˆåå°ä»»åŠ¡ï¼‰"""
    logger.info(f"ğŸ” å¼€å§‹æ‰§è¡Œæ–‡æ¡£åˆ†æ: {document_id}")
    
    try:
        # åˆ›å»ºåˆ†æå™¨
        analyzer = DocumentAnalyzer(config)
        
        # æ‰§è¡Œåˆ†æ
        result = analyzer.analyze_document_content(content)
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_result = {
            "analysis_id": analysis_id,
            "status": "completed",
            "started_at": datetime.now(),
            "completed_at": datetime.now(),
            "result": {
                "document_info": {
                    "title": result.document_info.title,
                    "version": result.document_info.version,
                    "description": result.document_info.description,
                    "servers": [{"url": server.url, "description": server.description} 
                              for server in result.document_info.servers]
                },
                "endpoints": [
                    {
                        "path": ep.path,
                        "method": ep.method,
                        "summary": ep.summary,
                        "description": ep.description,
                        "parameters": [
                            {
                                "name": param.name,
                                "location": param.location,
                                "type": param.type,
                                "required": param.required,
                                "description": param.description
                            }
                            for param in ep.parameters
                        ],
                        "responses": [
                            {
                                "status_code": resp.status_code,
                                "description": resp.description,
                                "content_type": resp.content_type
                            }
                            for resp in ep.responses
                        ]
                    }
                    for ep in result.endpoints
                ],
                "data_models": [
                    {
                        "name": model.name,
                        "type": model.type,
                        "properties": model.properties,
                        "required": model.required,
                        "description": model.description
                    }
                    for model in result.data_models
                ],
                "quality_score": result.quality_score,
                "issues": [
                    {
                        "type": issue.issue_type,
                        "severity": issue.severity,
                        "message": issue.message,
                        "location": issue.location
                    }
                    for issue in result.issues
                ]
            }
        }
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€
        documents_store[document_id]["status"] = "analyzed"
        documents_store[document_id]["analysis_result"] = analysis_result
        
        # ä¿å­˜åˆ°åˆ†æç»“æœå­˜å‚¨
        analysis_results_store[document_id] = result
        
        logger.info(f"âœ… æ–‡æ¡£åˆ†æå®Œæˆ: {document_id}")
        
    except Exception as e:
        logger.error(f"âŒ æ–‡æ¡£åˆ†æå¤±è´¥: {document_id} - {str(e)}")
        
        # ä¿å­˜é”™è¯¯ç»“æœ
        error_result = {
            "analysis_id": analysis_id,
            "status": "failed",
            "started_at": datetime.now(),
            "completed_at": datetime.now(),
            "error": {
                "type": type(e).__name__,
                "message": str(e)
            }
        }
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€
        documents_store[document_id]["status"] = "analysis_failed"
        documents_store[document_id]["analysis_result"] = error_result
